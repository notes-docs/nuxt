---
title: Stats Collection (统计数据收集)
description: ''
---

Scrapy 提供了一个方便的工具，用于以键/值形式收集统计数据，其中值通常是计数器。该工具称为统计数据收集器 (Stats Collector)，可以通过 **Crawler API** 的 `stats` 属性访问，如下面**常用统计数据收集器用法**部分中的示例所示。

然而，统计数据收集器始终可用，因此您可以随时在模块中导入它并使用其 API（递增或设置新的统计键），无论统计数据收集是否启用。如果禁用，API 仍将工作，但不会收集任何内容。这旨在简化统计数据收集器的使用：您应该只用一行代码来收集爬虫、Scrapy 扩展或您使用统计数据收集器的任何代码中的统计数据。

统计数据收集器的另一个特点是，它（启用时）非常高效，禁用时效率极高（几乎察觉不到）。

统计数据收集器为每个打开的爬虫维护一个统计数据表，该表在爬虫打开时自动打开，在爬虫关闭时自动关闭。

## Common Stats Collector uses (常用统计数据收集器用法)

通过 `stats` 属性访问统计数据收集器。这是一个访问统计数据的扩展示例：

```python
class ExtensionThatAccessStats:
    def __init__(self, stats):
        self.stats = stats

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.stats)
```

设置统计值：

```python
stats.set_value("hostname", socket.gethostname())
```

递增统计值：

```python
stats.inc_value("custom_count")
```

仅在大于前一个值时设置统计值：

```python
stats.max_value("max_items_scraped", value)
```

仅在小于前一个值时设置统计值：

```python
stats.min_value("min_free_memory_percent", value)
```

获取统计值：

```python
>>> stats.get_value("custom_count")
1
```

获取所有统计数据：

```python
>>> stats.get_stats()
{'custom_count': 1, 'start_time': datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}
```

## Available Stats Collectors (可用统计数据收集器)

除了基本的 **StatsCollector** 之外，Scrapy 中还有其他可用的统计数据收集器，它们扩展了基本的统计数据收集器。您可以通过 **STATS\_CLASS** 设置选择要使用的统计数据收集器。默认使用的统计数据收集器是 **MemoryStatsCollector**。

### `MemoryStatsCollector`

`class scrapy.statscollectors.MemoryStatsCollector[source]`

一个简单的统计数据收集器，它在爬虫关闭后将上次抓取运行（每个爬虫）的统计数据保留在内存中。可以通过 `spider_stats` 属性访问统计数据，它是一个以爬虫域名为键的字典。

这是 Scrapy 中使用的默认统计数据收集器。

#### `spider_stats`

一个字典的字典（以爬虫名称为键），包含每个爬虫上次抓取运行的统计数据。

### `DummyStatsCollector`

`class scrapy.statscollectors.DummyStatsCollector[source]`

一个不执行任何操作但效率极高的统计数据收集器（因为它不执行任何操作）。可以通过 **STATS\_CLASS** 设置此统计数据收集器，以禁用统计数据收集以提高性能。然而，统计数据收集的性能损失通常与 Scrapy 其他工作负载（如页面解析）相比微不足道。
