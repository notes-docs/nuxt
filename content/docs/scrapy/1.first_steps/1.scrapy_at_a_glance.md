---
title: Scrapy 概览
description: 'Scrapy（/ˈskreɪpaɪ/）是一个用于爬取网站和提取结构化数据的应用程序框架，可用于数据挖掘、信息处理或历史存档等广泛有用的应用。'
---

尽管 Scrapy 最初是为 [网络抓取](https://en.wikipedia.org/wiki/Web_scraping) 设计的，但它也可以用于使用 API（例如 [Amazon Associates Web Services](https://affiliate-program.amazon.com/welcome/ecs)）提取数据，或作为通用网络爬虫。

## 示例爬虫演练

为了向您展示 Scrapy 的优势，我们将通过一个 Scrapy 爬虫示例，使用最简单的方式来运行爬虫。

这是一个爬取网站 https://quotes.toscrape.com 上的名言的爬虫代码，并遵循分页：

```python
import scrapy

class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        "https://quotes.toscrape.com/tag/humor/",
    ]

    def parse(self, response):
        for quote in response.css("div.quote"):
            yield {
                "author": quote.xpath("span/small/text()").get(),
                "text": quote.css("span.text::text").get(),
            }

        next_page = response.css('li.next a::attr("href")').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)

```

将此代码放入一个文本文件，命名为 `quotes_spider.py`，然后使用 `runspider` 命令运行爬虫：

```bash
scrapy runspider quotes_spider.py -o quotes.jsonl
```

当它完成时，您将在 `quotes.jsonl` 文件中得到一个 JSON Lines 格式的名言列表，包含文本和作者，如下所示：

```json
{"author": "Jane Austen", "text": "\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d"}
{"author": "Steve Martin", "text": "\u201cA day without sunshine is like, you know, night.\u201d"}
{"author": "Garrison Keillor", "text": "\u201cAnyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.\u201d"}
...
```

### 发生了什么？

当您运行命令 `scrapy runspider quotes_spider.py` 时，Scrapy 在其中查找 Spider 定义并通过其爬虫引擎运行它。

爬取通过向 `start_urls` 属性中定义的 URL（在本例中，只有 humor 类别的名言 URL）发出请求来开始，并调用默认的回调方法 `parse`，将响应对象作为参数传递。在 `parse` 回调中，我们使用 CSS 选择器遍历 quote 元素，生成一个包含提取的名言文本和作者的 Python 字典，查找指向下一页的链接，并使用相同的 `parse` 方法作为回调来安排另一个请求。

在这里您会注意到 Scrapy 的主要优点之一：请求是 [异步调度和处理](https://docs.scrapy.org/en/latest/topics/architecture.html#topics-architecture) 的。这意味着 Scrapy 无需等待请求完成并处理，它可以在此期间发送另一个请求或执行其他操作。这也意味着即使请求失败或在处理过程中发生错误，其他请求也可以继续进行。

虽然这使您能够进行非常快速的爬取（以容错的方式同时发送多个并发请求），但 Scrapy 还允许您通过 [一些设置](https://docs.scrapy.org/en/latest/topics/settings.html#topics-settings-ref) 来控制爬取的 “礼貌性”。您可以设置每个请求之间的下载延迟，限制每个域或每个 IP 的并发请求数量，甚至 [使用自动节流扩展](https://docs.scrapy.org/en/latest/topics/autothrottle.html#topics-autothrottle) 来尝试自动确定这些设置。

::note
这里使用 [Feed 导出](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports) 来生成 JSON 文件，您可以轻松更改导出格式（例如 XML 或 CSV）或存储后端（例如 FTP 或 [Amazon S3](https://aws.amazon.com/s3/)）。您还可以编写一个 [Item Pipeline](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline) 将 Item 存储在数据库中。
::

### 还有什么？

您已经了解了如何使用 Scrapy 从网站中提取和存储 Item，但这只是冰山一角。Scrapy 提供了许多强大的功能，使抓取变得简单高效，例如：

* 内置支持使用扩展的 **CSS 选择器和 XPath 表达式** 从 HTML/XML 源中 [选择和提取](https://docs.scrapy.org/en/latest/topics/selectors.html#topics-selectors) 数据，并提供使用正则表达式提取的辅助方法。
* 一个 [交互式 shell 控制台](https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell)（支持 IPython），用于尝试 CSS 和 XPath 表达式来抓取数据，这在编写或调试爬虫时非常有用。
* 内置支持以多种格式（JSON、CSV、XML）[生成 Feed 导出](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports) 并将其存储在多个后端（FTP、S3、本地文件系统）中。
* 强大的编码支持和自动检测，用于处理外部、非标准和损坏的编码声明。
* [强大的可扩展性支持](https://docs.scrapy.org/en/latest/index.html#extending-scrapy)，允许您使用 [信号](https://docs.scrapy.org/en/latest/topics/signals.html#topics-signals) 和定义明确的 API（**中间件**、[扩展](https://docs.scrapy.org/en/latest/topics/extensions.html#topics-extensions) 和 [管道](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#topics-item-pipeline)）插入自己的功能。
* 广泛的内置扩展和中间件，用于处理：
  * cookie 和会话处理
  * HTTP 功能，如压缩、认证、缓存
  * 用户代理欺骗
  * robots.txt
  * 爬取深度限制
  * 等等
* 一个 [Telnet 控制台](https://docs.scrapy.org/en/latest/topics/telnetconsole.html#topics-telnetconsole)，用于连接到在 Scrapy 进程中运行的 Python 控制台，以自省和调试您的爬虫。
* 此外还有其他好东西，如可重用的爬虫，用于爬取 [站点地图](https://www.sitemaps.org/index.html) 和 XML/CSV Feed 中的站点，一个用于 [自动下载与抓取 Item 相关的图像](https://docs.scrapy.org/en/latest/topics/media-pipeline.html#topics-media-pipeline)（或任何其他媒体）的媒体管道，一个缓存 DNS 解析器，等等！

### 下一步是什么？

您的下一步是 [安装 Scrapy](https://docs.scrapy.org/en/latest/intro/install.html#intro-install)，[跟随教程](https://docs.scrapy.org/en/latest/intro/tutorial.html#intro-tutorial) 学习如何创建一个成熟的 Scrapy 项目，并 [加入社区](https://scrapy.org/community/)。感谢您的关注！
