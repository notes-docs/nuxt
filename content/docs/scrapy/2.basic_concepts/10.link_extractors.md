---
title: Link Extractors (链接提取器)
description: '链接提取器是一个从响应中提取链接的对象。'
---

**LxmlLinkExtractor** 的 `__init__` 方法接受决定哪些链接可以被提取的设置。**LxmlLinkExtractor.extract_links** 从 **Response** 对象返回匹配的 **Link** 对象列表。

链接提取器在 **CrawlSpider** 爬虫中通过一组 **Rule** 对象使用。

您也可以在常规爬虫中使用链接提取器。例如，您可以将 **LinkExtractor** 实例化为爬虫中的类变量，并从爬虫回调中使用它：

```python
def parse(self, response):
    for link in self.link_extractor.extract_links(response):
        yield Request(link.url, callback=self.parse)
```

## Link extractor reference (链接提取器参考)

链接提取器类是 `scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor`。为了方便起见，它也可以作为 `scrapy.linkextractors.LinkExtractor` 导入：

```python
from scrapy.linkextractors import LinkExtractor
```

### LxmlLinkExtractor

`class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor(allow=(), deny=(), allow_domains=(), deny_domains=(), deny_extensions=None, restrict_xpaths=(), restrict_css=(), tags=('a', 'area'), attrs=('href',), canonicalize=False, unique=True, process_value=None, strip=True)[source]`

**LxmlLinkExtractor** 是推荐的链接提取器，具有方便的过滤选项。它是使用 lxml 健壮的 HTMLParser 实现的。

**参数：**

* **allow** (`str` 或 `list`) – 一个正则表达式（或正则表达式列表），(绝对) URL 必须匹配才能被提取。如果未给定（或为空），它将匹配所有链接。
* **deny** (`str` 或 `list`) – 一个正则表达式（或正则表达式列表），(绝对) URL 必须匹配才能被排除（即不提取）。它优先于 `allow` 参数。如果未给定（或为空），它将不排除任何链接。
* **allow\_domains** (`str` 或 `list`) – 一个值或一个包含域的字符串列表，这些域将被考虑用于提取链接。
* **deny\_domains** (`str` 或 `list`) – 一个值或一个包含域的字符串列表，这些域将不被考虑用于提取链接。
* **deny\_extensions** (`list`) –
  一个值或一个包含在提取链接时应忽略的扩展名的字符串列表。如果未给定，它将默认为 `scrapy.linkextractors.IGNORED_EXTENSIONS`。
  2.0 版更改：`IGNORED_EXTENSIONS` 现在包括 `7z`、`7zip`、`apk`、`bz2`、`cdr`、`dmg`、`ico`、`iso`、`tar`、`tar.gz`、`webm` 和 `xz`。
* **restrict\_xpaths** (`str` 或 `list`) – 一个 XPath（或 XPath 列表），它定义响应中应从中提取链接的区域。如果给定，则只有由这些 XPath 选择的文本将被扫描以查找链接。
* **restrict\_css** (`str` 或 `list`) – 一个 CSS 选择器（或选择器列表），它定义响应中应从中提取链接的区域。与 `restrict_xpaths` 具有相同的行为。
* **restrict\_text** (`str` 或 `list`) – 一个正则表达式（或正则表达式列表），链接的文本必须匹配才能被提取。如果未给定（或为空），它将匹配所有链接。如果给定正则表达式列表，则如果链接至少匹配其中一个，则将提取该链接。
* **tags** (`str` 或 `list`) – 一个或多个在提取链接时要考虑的标签。默认为 `('a', 'area')`。
* **attrs** (`list`) – 一个或多个在查找要提取的链接时应考虑的属性（仅适用于 `tags` 参数中指定的那些标签）。默认为 `('href',)`。
* **canonicalize** (`bool`) – 是否规范化每个提取的 URL（使用 **w3lib.url.canonicalize\_url**）。默认为 `False`。请注意，`canonicalize_url` 用于重复检查；它可以更改服务器端可见的 URL，因此对于规范化 URL 和原始 URL 的请求，响应可能不同。如果您使用 LinkExtractor 来跟踪链接，则保持默认的 `canonicalize=False` 更健壮。
* **unique** (`bool`) – 是否对提取的链接应用重复过滤。
* **process\_value** (`collections.abc.Callable`) –
  一个函数，它接收从扫描的标签和属性中提取的每个值，可以修改该值并返回一个新值，或者返回 `None` 以完全忽略该链接。如果未给定，`process_value` 默认为 `lambda x: x`。
  例如，要从以下代码中提取链接：
  `<a href="javascript:goToPage('../other/page.html'); return false">Link text</a>`
  您可以在 `process_value` 中使用以下函数：
  ```python
  def process_value(value):
      m = re.search(r"javascript:goToPage\('(.*?)'", value)
      if m:
          return m.group(1)
  ```
* **strip** (`bool`) – 是否从提取的属性中去除空白字符。根据 HTML5 标准，`<a>`、`<area>` 和许多其他元素的 `href` 属性，`<img>`、`<iframe>` 元素的 `src` 属性等，都必须去除前导和尾随的空白字符，因此 LinkExtractor 默认会去除空格字符。设置为 `strip=False` 以关闭它（例如，如果您从允许前导/尾随空白字符的元素或属性中提取 URL）。

#### `extract_links(response: TextResponse) -> list[Link][source]`

从指定的 `response` 返回 **Link** 对象的列表。
只返回与传递给链接提取器 `__init__` 方法的设置匹配的链接。
如果 `unique` 属性设置为 `True`，则省略重复链接，否则将返回它们。

### Link

`class scrapy.link.Link(url: str, text: str = '', fragment: str = '', nofollow: bool = False)[source]`

**Link** 对象表示由 **LinkExtractor** 提取的链接。
使用以下锚点标签示例来说明参数：

`<a href="https://example.com/nofollow.html#foo" rel="nofollow">Dont follow this one</a>`

**参数：**

* `url` – 锚点标签中链接到的绝对 URL。从示例中，这是 https://example.com/nofollow.html 。
* `text` – 锚点标签中的文本。从示例中，这是 `Dont follow this one`。
* `fragment` – URL 中哈希符号后的部分。从示例中，这是 `foo`。
* `nofollow` – 锚点标签的 `rel` 属性中是否存在 nofollow 值的指示。
