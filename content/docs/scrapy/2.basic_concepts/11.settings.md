---
title: Settings (设置)
description: 'Scrapy 设置允许您自定义所有 Scrapy 组件的行为，包括核心、扩展、管道和爬虫本身。'
---

设置的基础设施提供了一个键值映射的全局命名空间，代码可以使用它来获取配置值。设置可以通过不同的机制填充，下面将对此进行描述。

设置也是选择当前活动的 Scrapy 项目的机制（如果您有多个项目）。

有关可用内置设置的列表，请参阅：**内置设置参考**。

## Designating the settings (指定设置)

使用 Scrapy 时，您必须告诉它您正在使用哪些设置。您可以通过使用环境变量 `SCRAPY_SETTINGS_MODULE` 来做到这一点。

`SCRAPY_SETTINGS_MODULE` 的值应采用 Python 路径语法，例如 `myproject.settings`。请注意，设置模块应位于 Python 的**导入搜索路径**中。

## Populating the settings (填充设置)

设置可以使用不同的机制填充，每种机制具有不同的优先级：

1.  **命令行设置**（最高优先级）
2.  **爬虫设置**
3.  **项目设置**
4.  **附加组件设置**
5.  **命令特定默认设置**
6.  **全局默认设置**（最低优先级）

### 1\. Command-line settings (命令行设置)

在命令行中设置的设置具有最高优先级，覆盖任何其他设置。

您可以使用 `-s`（或 `--set`）命令行选项明确覆盖一个或多个设置。

示例：

```bash
scrapy crawl myspider -s LOG_LEVEL=INFO -s LOG_FILE=scrapy.log
```

### 2\. Spider settings (爬虫设置)

**爬虫**可以定义自己的设置，这些设置将优先并覆盖项目设置。

**注意**

**预爬虫设置**不能按爬虫定义，当**在同一进程中运行多个爬虫**时，**Reactor 设置**不应按爬虫具有不同的值。

一种方法是设置它们的 `custom_settings` 属性：

```python
import scrapy

class MySpider(scrapy.Spider):
    name = "myspider"

    custom_settings = {
        "SOME_SETTING": "some value",
    }
```

通常最好实现 `update_settings()`，并且在那里设置的设置应该明确使用 `"spider"` 优先级：

```python
import scrapy

class MySpider(scrapy.Spider):
    name = "myspider"

    @classmethod
    def update_settings(cls, settings):
        super().update_settings(settings)
        settings.set("SOME_SETTING", "some value", priority="spider")
```

从 2.11 版新增。

也可以在 `from_crawler()` 方法中修改设置，例如根据**爬虫参数**或其他逻辑：

```python
import scrapy

class MySpider(scrapy.Spider):
    name = "myspider"

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        if "some_argument" in kwargs:
            spider.settings.set(
                "SOME_SETTING", kwargs["some_argument"], priority="spider"
            )
        return spider
```

### 3\. Project settings (项目设置)

Scrapy 项目包含一个设置模块，通常是一个名为 `settings.py` 的文件，您应该在该文件中填充适用于所有爬虫的大多数设置。

**另请参阅**

**指定设置**

### 4\. Add-on settings (附加组件设置)

**附加组件**可以修改设置。它们应尽可能以 `"addon"` 优先级执行此操作。

### 5\. Command-specific default settings (命令特定默认设置)

每个 **Scrapy 命令**都可以有自己的默认设置，这些设置会覆盖**全局默认设置**。

这些命令特定默认设置在每个命令类的 `default_settings` 属性中指定。

### 6\. Default global settings (默认全局设置)

`scrapy.settings.default_settings` 模块定义了一些**内置设置**的全局默认值。

**注意**

**startproject** 生成一个 `settings.py` 文件，该文件将某些设置设置为不同的值。
设置的参考文档指明了默认值（如果存在）。如果 **startproject** 设置了一个值，则该值被记录为默认值，而来自 `scrapy.settings.default_settings` 的值则被记录为“备用”。

## Compatibility with pickle (与 pickle 的兼容性)

设置值必须是**可序列化的**。

## Import paths and classes (导入路径和类)

从 2.4.0 版开始新增。

当一个设置引用一个由 Scrapy 导入的可调用对象（例如类或函数）时，您可以通过两种不同的方式指定该对象：

* 包含该对象导入路径的字符串
* 对象本身

例如：

```python
from mybot.pipelines.validate import ValidateMyItem

ITEM_PIPELINES = {
    # 传递类名...
    ValidateMyItem: 300,
    # ...等同于传递类路径
    "mybot.pipelines.validate.ValidateMyItem": 300,
}
```

**注意**

不支持传递不可调用对象。

## How to access settings (如何访问设置)

在爬虫中，可以通过 `self.settings` 访问设置：

```python
class MySpider(scrapy.Spider):
    name = "myspider"
    start_urls = ["http://example.com"]

    def parse(self, response):
        print(f"Existing settings: {self.settings.attributes.keys()}")
```

**注意**

`settings` 属性在爬虫初始化后在基础 Spider 类中设置。如果您想在初始化之前（例如，在您的爬虫的 `__init__()` 方法中）使用设置，您需要覆盖 `from_crawler()` 方法。

**组件**也可以**访问设置**。

`settings` 对象可以像 `dict` 一样使用（例如 `settings["LOG_ENABLED"]`）。但是，为了支持非字符串设置值（可能从命令行作为字符串传递），建议使用 **Settings API** 提供的方法之一。

## Component priority dictionaries (组件优先级字典)

**组件优先级字典**是一个 `dict`，其中键是**组件**，值是组件优先级。例如：

```python
{
    "path.to.ComponentA": None,
    ComponentB: 100,
}
```

组件可以指定为类对象或通过导入路径。

**警告**

组件优先级字典是常规的 `dict` 对象。请注意不要多次定义同一个组件，例如使用不同的导入路径字符串或同时定义导入路径和类型对象。

优先级可以是 `int` 或 `None`。

优先级为 1 的组件**在**优先级为 2 的组件之前。但是，“在之前”意味着什么取决于相应的设置。例如，在 **DOWNLOADER\_MIDDLEWARES** 设置中，组件的 `process_request()` 方法在后面组件的 `process_request()` 方法之前执行，但它们的 `process_response()` 方法在后面组件的 `process_response()` 方法之后执行。

优先级为 `None` 的组件被禁用。

一些组件优先级字典与某些内置值合并。例如，**DOWNLOADER\_MIDDLEWARES** 与 **DOWNLOADER\_MIDDLEWARES\_BASE** 合并。这就是 `None` 派上用场的地方，允许您在常规设置中禁用基本设置中的组件：

```python
DOWNLOADER_MIDDLEWARES = {
    "scrapy.downloadermiddlewares.offsite.OffsiteMiddleware": None,
}
```

## Special settings (特殊设置)

以下设置与其他所有设置略有不同。

### Pre-crawler settings (预爬虫设置)

**预爬虫设置**是在创建 **Crawler** 对象之前使用的设置。
这些设置**不能**从爬虫中设置。
这些设置是 **SPIDER\_LOADER\_CLASS** 以及相应**组件**使用的设置，例如默认组件的 **SPIDER\_MODULES** 和 **SPIDER\_LOADER\_WARN\_ONLY**。

### Reactor settings (Reactor 设置)

**Reactor 设置**是与 **Twisted reactor** 绑定的设置。
这些设置可以从爬虫中定义。但是，由于每个进程只能使用一个 reactor，因此当**在同一进程中运行多个爬虫**时，这些设置不能按爬虫使用不同的值。
通常，如果不同的爬虫定义了不同的值，则使用第一个定义的值。但是，如果两个爬虫请求不同的 reactor，则会引发异常。
这些设置是：

* **ASYNCIO\_EVENT\_LOOP**
* **DNS\_RESOLVER** 以及相应组件使用的设置，例如默认组件的 **DNSCACHE\_ENABLED**、**DNSCACHE\_SIZE** 和 **DNS\_TIMEOUT**。
* **REACTOR\_THREADPOOL\_MAXSIZE**
* **TWISTED\_REACTOR**

**ASYNCIO\_EVENT\_LOOP** 和 **TWISTED\_REACTOR** 在安装 reactor 时使用。其余设置在启动 reactor 时应用。

## 内置设置参考

https://docs.scrapy.org/en/latest/topics/settings.html#built-in-settings-reference
