---
title: 命令行工具
description: ''
---

Scrapy 通过 `scrapy` 命令行工具进行控制，这里将其称为 “Scrapy 工具”，以区别于子命令，子命令我们简称为 “命令” 或 “Scrapy 命令”。

Scrapy 工具提供多种命令，用于多种目的，每个命令都接受不同的参数和选项。

（`scrapy deploy` 命令已在 1.0 版本中移除，取而代之的是独立的 `scrapyd-deploy`。请参阅 [部署您的项目](https://scrapyd.readthedocs.io/en/latest/deploy.html)。）

## 配置设置

Scrapy 会在标准位置的 ini 风格的 `scrapy.cfg` 文件中查找配置参数：

* `/etc/scrapy.cfg` 或 `c:\scrapy\scrapy.cfg`（系统范围），
* `~/.config/scrapy.cfg`（`$XDG_CONFIG_HOME`）和 `~/.scrapy.cfg`（`$HOME`）用于全局（用户范围）设置，以及
* Scrapy 项目根目录下的 `scrapy.cfg`（参见下一节）。

这些文件中的设置按列出的优先级顺序合并：用户定义的值优先于系统范围的默认值，并且项目范围的设置（如果定义）将覆盖所有其他设置。

Scrapy 还理解并可以通过许多环境变量进行配置。目前这些变量是：

* `SCRAPY_SETTINGS_MODULE`（参见 [指定设置](https://docs.scrapy.org/en/latest/topics/settings.html#topics-settings-module-envvar)）
* `SCRAPY_PROJECT`（参见 [在项目之间共享根目录](https://docs.scrapy.org/en/latest/topics/commands.html#topics-project-envvar)）
* `SCRAPY_PYTHON_SHELL`（参见 [Scrapy shell](https://docs.scrapy.org/en/latest/topics/shell.html#topics-shell)）

## Scrapy 项目的默认结构

在深入了解命令行工具及其子命令之前，让我们首先了解 Scrapy 项目的目录结构。

尽管可以修改，但所有 Scrapy 项目默认都具有相同的文件结构，类似于：

```
scrapy.cfg
myproject/
    __init__.py
    items.py
    middlewares.py
    pipelines.py
    settings.py
    spiders/
        __init__.py
        spider1.py
        spider2.py
        ...
```

`scrapy.cfg` 文件所在的目录称为 **项目根目录**。该文件包含定义项目设置的 Python 模块的名称。这是一个示例：

```ini
[settings]
default = myproject.settings
```

## 在项目之间共享根目录

包含 `scrapy.cfg` 的项目根目录可以由多个 Scrapy 项目共享，每个项目都有自己的设置模块。

在这种情况下，您必须在 `scrapy.cfg` 文件的 `[settings]` 下为这些设置模块定义一个或多个别名：

```ini
[settings]
default = myproject1.settings
project1 = myproject1.settings
project2 = myproject2.settings
```

默认情况下，`scrapy` 命令行工具将使用 `default` 设置。使用 `SCRAPY_PROJECT` 环境变量为 `scrapy` 指定不同的项目：

```bash
$ scrapy settings --get BOT_NAME
Project 1 Bot
$ export SCRAPY_PROJECT=project2
$ scrapy settings --get BOT_NAME
Project 2 Bot
```

## 使用 `scrapy` 工具

您可以从不带任何参数运行 Scrapy 工具开始，它将打印一些使用帮助和可用命令：

```
Scrapy X.Y - no active project

Usage:
  scrapy <command> [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
```

如果您在 Scrapy 项目内部运行，第一行将打印当前活动的模块。在此示例中，它是在项目外部运行的。如果在项目内部运行，它将打印类似以下内容：

```
Scrapy X.Y - project: myproject

Usage:
  scrapy <command> [options] [args]

[...]
```

### 创建项目

您通常使用 `scrapy` 工具做的第一件事是创建您的 Scrapy 项目：

```bash
scrapy startproject myproject [project_dir]
```

这将在 `project_dir` 目录下创建一个 Scrapy 项目。如果未指定 `project_dir`，则 `project_dir` 将与 `myproject` 相同。

接下来，进入新项目目录：

```bash
cd project_dir
```

然后您就可以从那里使用 `scrapy` 命令来管理和控制您的项目了。

### 控制项目

您在项目内部使用 `scrapy` 工具来控制和管理它们。

例如，要创建一个新的爬虫：

```bash
scrapy genspider mydomain mydomain.com
```

一些 Scrapy 命令（如 `crawl`）必须在 Scrapy 项目内部运行。有关哪些命令必须在项目内部运行，哪些不需要的更多信息，请参见下面的 [命令参考](https://docs.scrapy.org/en/latest/topics/commands.html#topics-commands-ref)。

另请记住，在项目内部运行某些命令时，它们的行为可能略有不同。例如，如果正在抓取的 URL 与某个特定爬虫相关联，则 `fetch` 命令将使用爬虫重写行为（例如 `user_agent` 属性来重写用户代理）。这是故意的，因为 `fetch` 命令旨在用于检查爬虫如何下载页面。

## 可用工具命令

本节包含可用内置命令的列表，其中包含描述和一些用法示例。请记住，您始终可以通过运行以下命令获取有关每个命令的更多信息：

```bash
scrapy <command> -h
```

您可以通过以下命令查看所有可用命令：

```bash
scrapy -h
```

有两种命令：那些只在 Scrapy 项目内部工作的命令（项目特定命令）和那些也可以在没有活动 Scrapy 项目的情况下工作的命令（全局命令），尽管它们在项目内部运行时可能行为略有不同（因为它们会使用项目重写的设置）。

**全局命令：**

* `startproject`
* `genspider`
* `settings`
* `runspider`
* `shell`
* `fetch`
* `view`
* `version`

**仅项目命令：**

* `crawl`
* `check`
* `list`
* `edit`
* `parse`
* `bench`

### `startproject`

* 语法：`scrapy startproject <project_name> [project_dir]`
* 需要项目：否

在 `project_dir` 目录下创建一个名为 `project_name` 的新 Scrapy 项目。如果未指定 `project_dir`，则 `project_dir` 将与 `project_name` 相同。

用法示例：

```bash
$ scrapy startproject myproject
```

### `genspider`

* 语法：`scrapy genspider [-t template] <name> <domain or URL>`
* 需要项目：否

版本 2.6.0 新增：能够传递 URL 而不是域名。

在当前文件夹中或在当前项目的 `spiders` 文件夹中创建一个新爬虫（如果从项目内部调用）。`<name>` 参数被设置为爬虫的 `name`，而 `<domain or URL>` 用于生成 `allowed_domains` 和 `start_urls` 爬虫属性。

用法示例：

```bash
$ scrapy genspider -l
Available templates:
  basic
  crawl
  csvfeed
  xmlfeed

$ scrapy genspider example example.com
Created spider 'example' using template 'basic'

$ scrapy genspider -t crawl scrapyorg scrapy.org
Created spider 'scrapyorg' using template 'crawl'
```

这只是一个方便的快捷命令，用于基于预定义模板创建爬虫，但绝不是创建爬虫的唯一方法。您可以自己创建爬虫源代码文件，而不是使用此命令。

### `crawl`

* 语法：`scrapy crawl <spider>`
* 需要项目：是

使用爬虫开始爬取。

支持的选项：

* `-h`, `--help`：显示帮助信息并退出
* `-a NAME=VALUE`：设置爬虫参数（可重复）
* `--output FILE` 或 `-o FILE`：将抓取的 Item 附加到 FILE 的末尾（使用 `-` 表示标准输出）。要定义输出格式，请在输出 URI 的末尾设置冒号（即 `-o FILE:FORMAT`）
* `--overwrite-output FILE` 或 `-O FILE`：将抓取的 Item 导入 FILE，覆盖任何现有文件。要定义输出格式，请在输出 URI 的末尾设置冒号（即 `-O FILE:FORMAT`）

用法示例：

```bash
$ scrapy crawl myspider
[ ... myspider starts crawling ... ]

$ scrapy crawl -o myfile:csv myspider
[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]

$ scrapy crawl -O myfile:json myspider
[ ... myspider starts crawling and saves the result in myfile in json format overwriting the original content... ]
```

### `check`

* 语法：`scrapy check [-l] <spider>`
* 需要项目：是

运行契约检查。

用法示例：

```bash
$ scrapy check -l
first_spider
  * parse
  * parse_item
second_spider
  * parse
  * parse_item

$ scrapy check
[FAILED] first_spider:parse_item
>>> 'RetailPricex' field is missing

[FAILED] first_spider:parse
>>> Returned 92 requests, expected 0..4
```

### `list`

* 语法：`scrapy list`
* 需要项目：是

列出当前项目中所有可用的爬虫。输出是一行一个爬虫。

用法示例：

```bash
$ scrapy list
spider1
spider2
```

### `edit`

* 语法：`scrapy edit <spider>`
* 需要项目：是

使用 `EDITOR` 环境变量中定义的编辑器（如果未设置，则使用 `EDITOR` 设置）编辑给定的爬虫。

此命令仅作为最常见情况的便捷快捷方式提供，开发人员当然可以自由选择任何工具或 IDE 来编写和调试爬虫。

用法示例：

```bash
$ scrapy edit spider1
```

### `fetch`

* 语法：`scrapy fetch <url>`
* 需要项目：否

使用 Scrapy 下载器下载给定 URL 并将内容写入标准输出。

此命令的有趣之处在于它以爬虫下载页面的方式抓取页面。例如，如果爬虫具有覆盖 User Agent 的 `USER_AGENT` 属性，它将使用该属性。

因此，此命令可用于“查看”您的爬虫将如何抓取某个页面。

如果在项目外部使用，将不应用任何特定的每个爬虫行为，它将只使用默认的 Scrapy 下载器设置。

支持的选项：

* `--spider=SPIDER`：绕过爬虫自动检测并强制使用特定爬虫
* `--headers`：打印响应的 HTTP 头而不是响应体
* `--no-redirect`：不遵循 HTTP 3xx 重定向（默认是遵循它们）

用法示例：

```bash
$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]

$ scrapy fetch --nolog --headers http://www.example.com/
{'Accept-Ranges': ['bytes'],
 'Age': ['1263   '],
 'Connection': ['close     '],
 'Content-Length': ['596'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
 'Etag': ['"573c1-254-48c9c87349680"'],
 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
 'Server': ['Apache/2.2.3 (CentOS)']}
```

### `view`

* 语法：`scrapy view <url>`
* 需要项目：否

在浏览器中打开给定 URL，如同您的 Scrapy 爬虫“看到”的那样。有时爬虫看到的页面与普通用户不同，因此这可用于检查爬虫“看到”的内容并确认是否符合您的预期。

支持的选项：

* `--spider=SPIDER`：绕过爬虫自动检测并强制使用特定爬虫
* `--no-redirect`：不遵循 HTTP 3xx 重定向（默认是遵循它们）

用法示例：

```bash
$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
```

### `shell`

* 语法：`scrapy shell [url]`
* 需要项目：否

启动给定 URL（如果给定）的 Scrapy shell，如果没有给定 URL 则为空。也支持 UNIX 风格的本地文件路径，可以是相对路径（带 `./` 或 `../` 前缀）或绝对路径。有关详细信息，请参见 **Scrapy shell**。

支持的选项：

* `--spider=SPIDER`：绕过爬虫自动检测并强制使用特定爬虫
* `-c code`：在 shell 中执行代码，打印结果并退出
* `--no-redirect`：不遵循 HTTP 3xx 重定向（默认是遵循它们）；这仅影响您在命令行中作为参数传递的 URL；一旦您进入 shell，`fetch(url)` 仍将默认遵循 HTTP 重定向。

用法示例：

```bash
$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]

$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'
(200, 'http://www.example.com/')

# shell 默认遵循 HTTP 重定向
$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(200, 'http://example.com/')

# 您可以使用 --no-redirect 禁用此功能
# （仅适用于作为命令行参数传递的 URL）
$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')
```

### `parse`

* 语法：`scrapy parse <url> [options]`
* 需要项目：是

抓取给定 URL 并使用处理它的爬虫进行解析，使用通过 `--callback` 选项传递的方法，如果未给定则使用 `parse`。

支持的选项：

* `--spider=SPIDER`：绕过爬虫自动检测并强制使用特定爬虫
* `--a NAME=VALUE`：设置爬虫参数（可重复）
* `--callback` 或 `-c`：用作解析响应回调的爬虫方法
* `--meta` 或 `-m`：将传递给回调请求的额外请求元数据。这必须是有效的 JSON 字符串。示例：`--meta='{"foo" : "bar"}'`
* `--cbkwargs`：将传递给回调的额外关键字参数。这必须是有效的 JSON 字符串。示例：`--cbkwargs='{"foo" : "bar"}'`
* `--pipelines`：通过管道处理 Item
* `--rules` 或 `-r`：使用 **CrawlSpider** 规则来发现用于解析响应的回调（即爬虫方法）
* `--noitems`：不显示抓取的 Item
* `--nolinks`：不显示提取的链接
* `--nocolour`：避免使用 pygments 对输出进行着色
* `--depth` 或 `-d`：请求应递归跟踪的深度级别（默认值：1）
* `--verbose` 或 `-v`：显示每个深度级别的信息
* `--output` 或 `-o`：将抓取的 Item 导出到文件

版本 2.3 新增。

用法示例：

```bash
$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]

>>> STATUS DEPTH LEVEL 1 <<<
# Scraped Items  ------------------------------------------------------------
[{'name': 'Example item',
 'category': 'Furniture',
 'length': '12 cm'}]

# Requests  -----------------------------------------------------------------
[]
```

### `settings`

* 语法：`scrapy settings [options]`
* 需要项目：否

获取 Scrapy 设置的值。

如果在项目内部使用，它将显示项目设置值，否则它将显示该设置的默认 Scrapy 值。

用法示例：

```bash
$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
```

### `runspider`

* 语法：`scrapy runspider <spider_file.py>`
* 需要项目：否

运行 Python 文件中自包含的爬虫，无需创建项目。

用法示例：

```bash
$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
```

### `version`

* 语法：`scrapy version [-v]`
* 需要项目：否

打印 Scrapy 版本。如果与 `-v` 一起使用，它还会打印 Python、Twisted 和平台信息，这对于错误报告很有用。

### `bench`

* 语法：`scrapy bench`
* 需要项目：否

运行快速基准测试。**基准测试**。

## 自定义项目命令

您还可以使用 **COMMANDS_MODULE** 设置添加您的自定义项目命令。请参阅 `scrapy/commands` 中的 Scrapy 命令示例，了解如何实现您的命令。

### `COMMANDS_MODULE`

默认值：`''`（空字符串）

用于查找自定义 Scrapy 命令的模块。这用于为您的 Scrapy 项目添加自定义命令。

示例：

```python
COMMANDS_MODULE = "mybot.commands"
```

### 通过 `setup.py` 入口点注册命令

您还可以通过在库的 `setup.py` 文件的入口点中添加 `scrapy.commands` 部分来从外部库添加 Scrapy 命令。

以下示例添加了 `my_command` 命令：

```python
from setuptools import setup, find_packages

setup(
    name="scrapy-mymodule",
    entry_points={
        "scrapy.commands": [
            "my_command=my_scrapy_module.commands:MyCommand",
        ],
    },
)
```
