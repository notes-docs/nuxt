---
title: Jobs pausing and resuming crawls (作业：暂停和恢复爬取)
description: ''
---

有时，对于大型网站，需要暂停爬取并能够在以后恢复。

Scrapy 通过提供以下功能，开箱即用地支持此功能：

* 一个将已调度请求持久化到磁盘的调度器
* 一个将已访问请求持久化到磁盘的去重过滤器
* 一个在批次之间保持某些爬虫状态（键/值对）持久化的扩展

## Job directory (作业目录)

要启用持久化支持，您只需通过 **JOBDIR** 设置定义一个**作业目录**。此目录将用于存储保持单个作业（即爬虫运行）状态所需的所有数据。重要的是要注意，此目录不得由不同的爬虫共享，甚至不能由同一爬虫的不同作业/运行共享，因为它旨在用于存储**单个**作业的状态。

## How to use it (如何使用)

要启动一个启用持久化支持的爬虫，请像这样运行它：

```bash
scrapy crawl somespider -s JOBDIR=crawls/somespider-1
```

然后，您可以随时安全地停止爬虫（通过按 Ctrl-C 或发送信号），并稍后通过发出相同的命令来恢复它：

```bash
scrapy crawl somespider -s JOBDIR=crawls/somespider-1
```

## Keeping persistent state between batches (在批次之间保持持久状态)

有时，您会希望在暂停/恢复批次之间保持一些持久的爬虫状态。您可以使用 `spider.state` 属性来实现，它应该是一个字典。有一个**内置扩展**负责在爬虫启动和停止时从作业目录中序列化、存储和加载该属性。

以下是一个使用爬虫状态的回调示例（为简洁起见，省略了其他爬虫代码）：

```python
def parse_item(self, response):
    # parse item here
    self.state["items_count"] = self.state.get("items_count", 0) + 1
```

## Persistence gotchas (持久化陷阱)

如果您想使用 Scrapy 持久化支持，需要记住以下几点：

### Cookies expiration (Cookie 过期)

Cookie 可能会过期。因此，如果您不尽快恢复爬虫，已调度的请求可能不再有效。如果您的爬虫不依赖 Cookie，则不会出现此问题。

### Request serialization (请求序列化)

为了使持久化工作，**Request** 对象必须可以使用 **pickle** 进行序列化，但传递给其 `__init__` 方法的 `callback` 和 `errback` 值除外，它们必须是正在运行的 **Spider** 类的**方法**。

如果您希望记录无法序列化的请求，可以将项目设置中的 **SCHEDULER\_DEBUG** 设置为 `True`。默认情况下为 `False`。
